{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "68dacc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0),\n",
       " StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b2a05a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6b50c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    pickle_path = '../data/mnist/data.pkl'\n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    mnist = fetch_openml(name='mnist_784', version=1, parser=\"auto\")\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "    return mnist\n",
    "\n",
    "mnist = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "861baa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from jax import numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "all_features = mnist['data']\n",
    "all_targets = mnist['target']\n",
    "\n",
    "bool_vec = [i < 60_000 for i in range(len(all_targets))]\n",
    "shuffle(bool_vec)\n",
    "split_df = lambda df : (\n",
    "    df[pd.Series(bool_vec).values],\n",
    "    df[pd.Series([not b for b in bool_vec]).values]\n",
    ")\n",
    "\n",
    "train_features, test_features = split_df(all_features)\n",
    "train_targets, test_targets = split_df(all_targets)\n",
    "\n",
    "format_jnp = lambda *dfs : tuple([jnp.asarray(df.to_numpy(), dtype='float32') for df in dfs])\n",
    "\n",
    "ftr_train, ftr_test, tgt_train, tgt_test = format_jnp(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    pd.get_dummies(train_targets),\n",
    "    pd.get_dummies(test_targets)\n",
    ")\n",
    "\n",
    "normalize = lambda ftr_df : ftr_df / jnp.linalg.norm(ftr_df, axis=1, keepdims=True)\n",
    "norm_ftr_train, norm_ftr_test = normalize(ftr_train), normalize(ftr_test)\n",
    "\n",
    "\"\"\"\n",
    "Quick sanity check to make sure that features have been properly normalized\n",
    "\"\"\"\n",
    "\n",
    "is_normalized = all(jnp.isclose(\n",
    "    jnp.linalg.norm(norm_ftr_train, axis=1),\n",
    "    jnp.full(norm_ftr_train.shape[0], 1.0)\n",
    "))\n",
    "\n",
    "assert(is_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22a31de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from jax import random as jrand\n",
    "\n",
    "class DistSampler:\n",
    "    def __init__(self):\n",
    "        self.key = jrand.PRNGKey(randint(0, 10**6))\n",
    "        \n",
    "    def normal(self, *shape):\n",
    "        self.key, subkey = jrand.split(self.key)\n",
    "        return jrand.normal(subkey, shape=tuple(shape))\n",
    "    \n",
    "    def uniform(self, *shape):\n",
    "        self.key, subkey = jrand.split(self.key)\n",
    "        return jrand.uniform(subkey, shape=tuple(shape))\n",
    "    \n",
    "dist_sampler = DistSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e29509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "         @@@@@@             \n",
      "        @@@@@@@@            \n",
      "        @@@@@@@@@           \n",
      "          @@@@@@@@          \n",
      "              @@@@          \n",
      "              @@@@          \n",
      "              @@@@          \n",
      "              @@@           \n",
      "             @@@@           \n",
      "           @@@@@@@          \n",
      "          @@@@@@@@@@        \n",
      "          @@@@@@@@@@@       \n",
      "          @@@@@@@@@@@@      \n",
      "           @@@@@@@@@        \n",
      "            @@@             \n",
      "           @@@@             \n",
      "          @@@@@             \n",
      "           @@@@             \n",
      "           @@@@             \n",
      "           @@@              \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "\n",
      "\n",
      "Answer: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "idx = randint(0, ftr_train.shape[0])\n",
    "x, y = ftr_train[idx], tgt_train[idx]\n",
    "\n",
    "img = jnp.reshape(x, (28,28))\n",
    "for row in img:\n",
    "    print(''.join([\n",
    "        '@' if pix > 100 else ' ' for pix in row\n",
    "    ]))\n",
    "    \n",
    "print(f'\\n\\nAnswer: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ed70e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, nn, jit\n",
    "\n",
    "num_obs, num_ftrs = ftr_train.shape\n",
    "\n",
    "def xlayer(x, w, b):\n",
    "    # x has dims (#In,) and w has dims (#Out, #In) and b has dims (#Out,)\n",
    "    return jnp.dot(w, x) + b\n",
    "\n",
    "def compose_layers(activations, index=0):\n",
    "    afn = activations[index]\n",
    "    if len(activations) - 1 == index:\n",
    "        return lambda x, params : afn(xlayer(x, params[2*index], params[2*index+1]))\n",
    "    else:\n",
    "        nextfn = compose_layers(activations, index + 1)\n",
    "        return lambda x, params : nextfn(\n",
    "            afn(xlayer(x, params[2*index], params[2*index+1])),\n",
    "            params\n",
    "        )\n",
    "\n",
    "def make_nn_and_params(*layers, in_dim=None, sampler=None):\n",
    "    activations, dims = list(zip(*layers))\n",
    "    composed_layers = compose_layers(activations)\n",
    "    nn_func = lambda x, params : nn.softmax(composed_layers(x, params))\n",
    "    \n",
    "    def make_params():\n",
    "        param_list = []\n",
    "        prev_dim = in_dim\n",
    "        for d in dims:\n",
    "            param_list.append(sampler(d, prev_dim)) # Wi\n",
    "            param_list.append(sampler(d)) # Bi\n",
    "            prev_dim = d\n",
    "        return param_list\n",
    "\n",
    "    return nn_func, make_params\n",
    "\n",
    "neural_net, make_params = make_nn_and_params(\n",
    "    (jnp.tanh, 30),\n",
    "    (jnp.tanh, 50),\n",
    "    (jnp.tanh, 10),\n",
    "    #(nn.sigmoid, 30),\n",
    "    #(nn.sigmoid, 10),\n",
    "    in_dim = 784,\n",
    "    sampler = dist_sampler.normal\n",
    ")\n",
    "\n",
    "# def neural_net(x, params):\n",
    "#     w1, b1, w2, b2 = params #, w3, b3 = params\n",
    "#     t0 = tanh_layer(x, w1, b1)\n",
    "#     t1 = tanh_layer(t0, w2, b2)\n",
    "#     #t2 = leaky_relu_layer(t1, w3, b3)\n",
    "#     return nn.softmax(t1)\n",
    "\n",
    "# params = [\n",
    "#     dist_sampler.normal(3000, num_ftrs), # w1\n",
    "#     dist_sampler.normal(3000), # b1\n",
    "# #     dist_sampler.uniform(50, 784), # w2\n",
    "# #     dist_sampler.uniform(50), # b2\n",
    "#     dist_sampler.normal(10, 3000), # w3\n",
    "#     dist_sampler.normal(10), # b3\n",
    "# ]\n",
    "\n",
    "def cross_entropy(prediction, truth):\n",
    "    return -truth * jnp.log(prediction) - (1. - truth)*jnp.log(1. - prediction)\n",
    "\n",
    "# Cross entropy\n",
    "def loss_fn(params, x, y):\n",
    "    out = neural_net(x, params)\n",
    "    cross_entropy_vec = cross_entropy(out, y)\n",
    "    ce = jnp.sum(cross_entropy_vec)\n",
    "    return ce\n",
    "\n",
    "# GOTCHA\n",
    "# by default, only the first paramter of the input function\n",
    "# will be differentiated against\n",
    "loss_gradient = jit(grad(loss_fn))\n",
    "jit_nn = jit(neural_net)\n",
    "jit_loss = jit(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2de6a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import vmap\n",
    "\n",
    "def make_loss_grad(params):\n",
    "    loss_grad = grad(loss_fn)\n",
    "    partial_loss_grad = partial(loss_grad, params)\n",
    "    vector_loss_grad = vmap(partial_loss_grad, (0, 0), 0)\n",
    "    return jit(vector_loss_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "752844d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "Obs, Resp = norm_ftr_train, tgt_train\n",
    "\n",
    "def random_rows(n, *arrs):\n",
    "    idxs = sample(list(range(arrs[0].shape[0])), n)\n",
    "    return [\n",
    "        jnp.take(x, jnp.asarray(idxs), axis=0) for x in arrs\n",
    "    ]\n",
    "\n",
    "def sgd_minibatch(params, batch_size, learning_rate=0.01):\n",
    "    new_params = [jnp.copy(p) for p in params]\n",
    "    for _ in range(batch_size):\n",
    "        idx = randint(0, num_obs - 1)\n",
    "        x, y = Obs[idx], Resp[idx]\n",
    "        grads = loss_gradient(params, x, y)\n",
    "        new_params = [\n",
    "            param_vec - ((learning_rate / batch_size) * grad)\n",
    "            for param_vec, grad in zip(new_params, grads)\n",
    "        ]\n",
    "    return new_params\n",
    "\n",
    "def sgd_vector_minibatch(params, batch_size, learning_rate=0.01):\n",
    "    loss_grad = make_loss_grad(params)\n",
    "    batch_x, batch_y = random_rows(batch_size, Obs, Resp)\n",
    "    param_gradients = loss_grad(batch_x, batch_y)\n",
    "    average_grad_per_param = [\n",
    "        jnp.average(gradients, 0)\n",
    "        for gradients in param_gradients\n",
    "    ]\n",
    "    updated_params = [\n",
    "        param - (learning_rate * average_grad)\n",
    "        for param, average_grad in zip(params, average_grad_per_param)\n",
    "    ]\n",
    "    return updated_params \n",
    "\n",
    "def avg_training_loss(params):\n",
    "    loss = 0\n",
    "    n = 50\n",
    "    for _ in range(n):\n",
    "        idx = randint(0, num_obs - 1)\n",
    "        x, y = norm_ftr_train[idx], tgt_train[idx]\n",
    "        loss += jit_loss(params, x, y)\n",
    "    return loss / n\n",
    "\n",
    "def _max_idx(vec):\n",
    "    max_x = vec[0]\n",
    "    max_i = 0\n",
    "    for i, x in enumerate(vec):\n",
    "        if vec[i] > max_x:\n",
    "            max_x = vec[i]\n",
    "            max_i = i\n",
    "    return max_i\n",
    "        \n",
    "def show_prediction_accuracy(params):\n",
    "    correct = 0\n",
    "    n = 1000\n",
    "    for idx in range(n):\n",
    "        x, y = norm_ftr_test[idx], tgt_test[idx]\n",
    "        pred_vec = jit_nn(x, params)\n",
    "        if _max_idx(pred_vec) == _max_idx(y):\n",
    "            correct += 1\n",
    "    print(f'{correct}/{n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5d8666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a206b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# class Clock():\n",
    "#     def __init__(self):\n",
    "#         self.start_time = time.time()\n",
    "        \n",
    "#     def start(self):\n",
    "#         self.start_time = time.time()\n",
    "    \n",
    "#     def stop(self):\n",
    "#         return time.time() - self.start_time\n",
    "    \n",
    "# clock = Clock()\n",
    "\n",
    "# clock.start()\n",
    "# for i, epoch_num in enumerate(range(100)):\n",
    "#     params = sgd_minibatch(params, 50, .5)\n",
    "#     if (i+1) % 20 == 0:\n",
    "#         print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "# print(f'minibatch time = {clock.stop()}')\n",
    "\n",
    "# clock.start()\n",
    "# for i, epoch_num in enumerate(range(100)):\n",
    "#     params = sgd_vector_minibatch(params, 50, .5)\n",
    "#     if (i+1) % 20 == 0:\n",
    "#         print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "# print(f'vector_minibatch time = {clock.stop()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e290054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100) average loss: 2.8394389152526855\n",
      "(200) average loss: 2.632817029953003\n",
      "(300) average loss: 2.053027868270874\n",
      "(400) average loss: 1.8389965295791626\n",
      "(500) average loss: 2.0067193508148193\n",
      "(600) average loss: 1.8944953680038452\n",
      "(700) average loss: 1.8663491010665894\n",
      "(800) average loss: 1.860311508178711\n",
      "(900) average loss: 1.717785358428955\n",
      "(1000) average loss: 1.8699570894241333\n",
      "861/1000\n",
      "(1100) average loss: 1.7482151985168457\n",
      "(1200) average loss: 1.866131067276001\n",
      "(1300) average loss: 1.701192855834961\n",
      "(1400) average loss: 1.625333547592163\n",
      "(1500) average loss: 1.633105754852295\n",
      "(1600) average loss: 1.9269949197769165\n",
      "(1700) average loss: 1.746794581413269\n",
      "(1800) average loss: 1.5866358280181885\n",
      "(1900) average loss: 1.6834129095077515\n",
      "(2000) average loss: 1.8284664154052734\n",
      "896/1000\n",
      "(2100) average loss: 1.7812992334365845\n",
      "(2200) average loss: 1.6934010982513428\n",
      "(2300) average loss: 1.584769368171692\n",
      "(2400) average loss: 1.6393221616744995\n",
      "(2500) average loss: 1.5406081676483154\n",
      "(2600) average loss: 1.528291940689087\n",
      "(2700) average loss: 1.8746123313903809\n",
      "(2800) average loss: 1.7880369424819946\n",
      "(2900) average loss: 1.4870717525482178\n",
      "(3000) average loss: 1.6423068046569824\n",
      "921/1000\n",
      "(3100) average loss: 1.597622036933899\n",
      "(3200) average loss: 1.528443455696106\n",
      "(3300) average loss: 1.6283847093582153\n",
      "(3400) average loss: 1.7240039110183716\n",
      "(3500) average loss: 1.6096035242080688\n",
      "(3600) average loss: 1.6268037557601929\n",
      "(3700) average loss: 1.4851053953170776\n",
      "(3800) average loss: 1.5318504571914673\n",
      "(3900) average loss: 1.628943920135498\n",
      "(4000) average loss: 1.670733094215393\n",
      "926/1000\n",
      "(4100) average loss: 1.5911335945129395\n",
      "(4200) average loss: 1.4684011936187744\n",
      "(4300) average loss: 1.4735420942306519\n",
      "(4400) average loss: 1.6212698221206665\n",
      "(4500) average loss: 1.4450652599334717\n",
      "(4600) average loss: 1.5073634386062622\n",
      "(4700) average loss: 1.5965921878814697\n",
      "(4800) average loss: 1.5137417316436768\n",
      "(4900) average loss: 1.4559050798416138\n",
      "(5000) average loss: 1.5052099227905273\n",
      "933/1000\n",
      "(5100) average loss: 1.4963756799697876\n",
      "(5200) average loss: 1.6116873025894165\n",
      "(5300) average loss: 1.5238206386566162\n",
      "(5400) average loss: 1.5530354976654053\n",
      "(5500) average loss: 1.389818787574768\n",
      "(5600) average loss: 1.5668588876724243\n",
      "(5700) average loss: 1.443542718887329\n",
      "(5800) average loss: 1.5689284801483154\n",
      "(5900) average loss: 1.5217832326889038\n",
      "(6000) average loss: 1.4600175619125366\n",
      "935/1000\n",
      "(6100) average loss: 1.5306479930877686\n",
      "(6200) average loss: 1.4944788217544556\n",
      "(6300) average loss: 1.5542994737625122\n",
      "(6400) average loss: 1.4340674877166748\n",
      "(6500) average loss: 1.482103705406189\n",
      "(6600) average loss: 1.4916220903396606\n",
      "(6700) average loss: 1.6776522397994995\n",
      "(6800) average loss: 1.5242730379104614\n",
      "(6900) average loss: 1.4228960275650024\n",
      "(7000) average loss: 1.4785517454147339\n",
      "943/1000\n",
      "(7100) average loss: 1.378238320350647\n",
      "(7200) average loss: 1.4913345575332642\n",
      "(7300) average loss: 1.5200132131576538\n",
      "(7400) average loss: 1.6558713912963867\n",
      "(7500) average loss: 1.6255910396575928\n",
      "(7600) average loss: 1.497347116470337\n",
      "(7700) average loss: 1.415958046913147\n",
      "(7800) average loss: 1.455919861793518\n",
      "(7900) average loss: 1.4169613122940063\n",
      "(8000) average loss: 1.5426934957504272\n",
      "943/1000\n",
      "(8100) average loss: 1.505266785621643\n",
      "(8200) average loss: 1.4266353845596313\n",
      "(8300) average loss: 1.5017000436782837\n",
      "(8400) average loss: 1.505936861038208\n",
      "(8500) average loss: 1.5578174591064453\n",
      "(8600) average loss: 1.6069778203964233\n",
      "(8700) average loss: 1.3818743228912354\n",
      "(8800) average loss: 1.5326894521713257\n",
      "(8900) average loss: 1.4407143592834473\n",
      "(9000) average loss: 1.5023603439331055\n",
      "944/1000\n",
      "(9100) average loss: 1.5312129259109497\n",
      "(9200) average loss: 1.6804258823394775\n",
      "(9300) average loss: 1.497387170791626\n",
      "(9400) average loss: 1.5139175653457642\n",
      "(9500) average loss: 1.5503264665603638\n",
      "(9600) average loss: 1.5205520391464233\n",
      "(9700) average loss: 1.4546771049499512\n",
      "(9800) average loss: 1.3985028266906738\n",
      "(9900) average loss: 1.48607337474823\n",
      "(10000) average loss: 1.4581875801086426\n",
      "942/1000\n"
     ]
    }
   ],
   "source": [
    "for i, epoch_num in enumerate(range(10_000)):\n",
    "    params = sgd_vector_minibatch(params, 60, .5)\n",
    "    #params = sgd_minibatch(params, 30, .5)\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "    if (i+1) % 1000 == 0:\n",
    "        show_prediction_accuracy(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6c87db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100) average loss: 1.4572159051895142\n",
      "(200) average loss: 1.490925669670105\n",
      "(300) average loss: 1.466302514076233\n",
      "(400) average loss: 1.4840394258499146\n",
      "(500) average loss: 1.4379446506500244\n",
      "(600) average loss: 1.5711660385131836\n",
      "(700) average loss: 1.4854987859725952\n",
      "(800) average loss: 1.3935836553573608\n",
      "(900) average loss: 1.603747010231018\n",
      "(1000) average loss: 1.467980980873108\n",
      "943/1000\n",
      "(1100) average loss: 1.6262611150741577\n",
      "(1200) average loss: 1.5837026834487915\n",
      "(1300) average loss: 1.4796726703643799\n",
      "(1400) average loss: 1.4464231729507446\n",
      "(1500) average loss: 1.582987904548645\n",
      "(1600) average loss: 1.5626029968261719\n",
      "(1700) average loss: 1.5450429916381836\n",
      "(1800) average loss: 1.393280267715454\n",
      "(1900) average loss: 1.6723823547363281\n",
      "(2000) average loss: 1.405208706855774\n",
      "945/1000\n",
      "(2100) average loss: 1.4778988361358643\n",
      "(2200) average loss: 1.5622962713241577\n",
      "(2300) average loss: 1.435713291168213\n",
      "(2400) average loss: 1.560678243637085\n",
      "(2500) average loss: 1.4793624877929688\n",
      "(2600) average loss: 1.5443283319473267\n",
      "(2700) average loss: 1.4317563772201538\n",
      "(2800) average loss: 1.5003471374511719\n",
      "(2900) average loss: 1.5834221839904785\n",
      "(3000) average loss: 1.5368309020996094\n",
      "944/1000\n",
      "(3100) average loss: 1.5200932025909424\n",
      "(3200) average loss: 1.5646002292633057\n",
      "(3300) average loss: 1.479266881942749\n",
      "(3400) average loss: 1.530324101448059\n",
      "(3500) average loss: 1.4937244653701782\n",
      "(3600) average loss: 1.529071569442749\n",
      "(3700) average loss: 1.581529974937439\n",
      "(3800) average loss: 1.3933762311935425\n",
      "(3900) average loss: 1.5190627574920654\n",
      "(4000) average loss: 1.5345170497894287\n",
      "943/1000\n",
      "(4100) average loss: 1.5309017896652222\n",
      "(4200) average loss: 1.5004888772964478\n",
      "(4300) average loss: 1.4635272026062012\n",
      "(4400) average loss: 1.4543342590332031\n",
      "(4500) average loss: 1.453956127166748\n",
      "(4600) average loss: 1.4644306898117065\n",
      "(4700) average loss: 1.5309758186340332\n",
      "(4800) average loss: 1.4309011697769165\n",
      "(4900) average loss: 1.4488368034362793\n",
      "(5000) average loss: 1.5232049226760864\n",
      "944/1000\n",
      "(5100) average loss: 1.5061012506484985\n",
      "(5200) average loss: 1.5567688941955566\n",
      "(5300) average loss: 1.5781770944595337\n",
      "(5400) average loss: 1.4923814535140991\n",
      "(5500) average loss: 1.4957019090652466\n",
      "(5600) average loss: 1.493664264678955\n",
      "(5700) average loss: 1.5553083419799805\n",
      "(5800) average loss: 1.537699818611145\n",
      "(5900) average loss: 1.5601626634597778\n",
      "(6000) average loss: 1.4688974618911743\n",
      "943/1000\n",
      "(6100) average loss: 1.4536961317062378\n",
      "(6200) average loss: 1.612884283065796\n",
      "(6300) average loss: 1.5084255933761597\n",
      "(6400) average loss: 1.590271234512329\n",
      "(6500) average loss: 1.4553595781326294\n",
      "(6600) average loss: 1.531239628791809\n",
      "(6700) average loss: 1.5973807573318481\n",
      "(6800) average loss: 1.5070834159851074\n",
      "(6900) average loss: 1.462010145187378\n",
      "(7000) average loss: 1.4970988035202026\n",
      "943/1000\n",
      "(7100) average loss: 1.4470832347869873\n",
      "(7200) average loss: 1.5274529457092285\n",
      "(7300) average loss: 1.4446419477462769\n",
      "(7400) average loss: 1.4418699741363525\n",
      "(7500) average loss: 1.4610551595687866\n",
      "(7600) average loss: 1.5039292573928833\n",
      "(7700) average loss: 1.4447126388549805\n",
      "(7800) average loss: 1.5741982460021973\n",
      "(7900) average loss: 1.402555227279663\n",
      "(8000) average loss: 1.4233711957931519\n",
      "943/1000\n",
      "(8100) average loss: 1.5459922552108765\n",
      "(8200) average loss: 1.487252950668335\n",
      "(8300) average loss: 1.503677487373352\n",
      "(8400) average loss: 1.411551594734192\n",
      "(8500) average loss: 1.545033574104309\n",
      "(8600) average loss: 1.5885676145553589\n",
      "(8700) average loss: 1.4006539583206177\n",
      "(8800) average loss: 1.4378893375396729\n",
      "(8900) average loss: 1.416371464729309\n",
      "(9000) average loss: 1.441149115562439\n",
      "943/1000\n",
      "(9100) average loss: 1.4116219282150269\n",
      "(9200) average loss: 1.456414818763733\n",
      "(9300) average loss: 1.4653865098953247\n",
      "(9400) average loss: 1.4916176795959473\n",
      "(9500) average loss: 1.5690984725952148\n",
      "(9600) average loss: 1.4566195011138916\n",
      "(9700) average loss: 1.402393102645874\n",
      "(9800) average loss: 1.5321009159088135\n",
      "(9900) average loss: 1.4234613180160522\n",
      "(10000) average loss: 1.4613213539123535\n",
      "943/1000\n"
     ]
    }
   ],
   "source": [
    "for i, epoch_num in enumerate(range(10_000)):\n",
    "    params = sgd_vector_minibatch(params, 60, .005)\n",
    "    #params = sgd_minibatch(params, 30, .5)\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "    if (i+1) % 1000 == 0:\n",
    "        show_prediction_accuracy(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616ca96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609eab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b3f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nns",
   "language": "python",
   "name": "nns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68dacc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0),\n",
       " StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698a883",
   "metadata": {},
   "source": [
    "# *Solving MNIST with JAX*\n",
    "\n",
    "JAX is a cool library. Among other things, it:\n",
    "- can JIT compile code for a CPU/GPU/TPU/etc...\n",
    "- makes parallel execution easy, even on separate devices\n",
    "- can transform a scalar-valued function into one that computes its gradient\n",
    "\n",
    "What better way to explore this library than with a neural net?\n",
    "\n",
    "Special thanks to [You Don't Know JAX](https://colinraffel.com/blog/you-don-t-know-jax.html) and [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f65f4",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "We can load the MNIST dataset into a pandas dataframe via OpenML and sklearn's OpenML interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b50c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_mnist():\n",
    "    pickle_path = '../data/mnist/data.pkl'\n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    mnist = fetch_openml(name='mnist_784', version=1, parser='auto')\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "    return mnist\n",
    "\n",
    "mnist = load_mnist()\n",
    "all_features, all_targets = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861baa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image\n",
      "\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "            @@@@@@@         \n",
      "          @@@@@@@@@@@       \n",
      "         @@@@@@@@@@@@@      \n",
      "         @@@@@@@@@@@@@      \n",
      "        @@@@@@   @@@@       \n",
      "         @@@@@   @@@@       \n",
      "          @@@@@@@@@@        \n",
      "          @@@@@@@@@@        \n",
      "           @@@@@@@@         \n",
      "            @@@@@@          \n",
      "            @@@@@           \n",
      "            @@@@@           \n",
      "           @@@@@@           \n",
      "          @@@@@@@@          \n",
      "          @@@@@@@@          \n",
      "         @@@@@@@@           \n",
      "        @@@@@@@@@           \n",
      "        @@@@@@@@            \n",
      "        @@@@@@@@            \n",
      "         @@@@               \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "\n",
      "Sample answer: 8\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle, randint\n",
    "from jax import numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Split the data into train and test segments, then format it as JAX matrices\n",
    "\"\"\"\n",
    "bool_vec = [i < 60_000 for i in range(len(all_targets))]\n",
    "shuffle(bool_vec)\n",
    "split_df = lambda df : (\n",
    "    df[pd.Series(bool_vec).values],\n",
    "    df[pd.Series([not b for b in bool_vec]).values]\n",
    ")\n",
    "\n",
    "train_features, test_features = split_df(all_features)\n",
    "train_targets, test_targets = split_df(all_targets)\n",
    "\n",
    "format_jnp = lambda *dfs : tuple([jnp.asarray(df.to_numpy(), dtype='float32') for df in dfs])\n",
    "\n",
    "ftr_train, ftr_test, tgt_train, tgt_test = format_jnp(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    pd.get_dummies(train_targets),\n",
    "    pd.get_dummies(test_targets)\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Spot check: print a rough sketch of a sample number, along with the expected answer.\n",
    "\"\"\"\n",
    "print('Sample image\\n')\n",
    "\n",
    "idx = randint(0, ftr_train.shape[0])\n",
    "x, y = ftr_train[idx], tgt_train[idx]\n",
    "\n",
    "img = jnp.reshape(x, (28,28))\n",
    "for row in img:\n",
    "    print(''.join(['@' if pix > 100 else ' ' for pix in row]))\n",
    "    \n",
    "print(f'\\nSample answer: {jnp.argmax(y)}')\n",
    "\n",
    "\"\"\"\n",
    "Normalize the features\n",
    "\"\"\"\n",
    "normalize = lambda ftr_df : ftr_df / jnp.linalg.norm(ftr_df, axis=1, keepdims=True)\n",
    "ftr_train, ftr_test = normalize(ftr_train), normalize(ftr_test)\n",
    "Obs, Resp = ftr_train, tgt_train\n",
    "TestObs, TestResp = ftr_test, tgt_test\n",
    "# quick sanity check\n",
    "assert all(jnp.isclose(jnp.linalg.norm(Obs, axis=1), jnp.full(Obs.shape[0], 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a31de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from jax import random as jrand\n",
    "\n",
    "class DistSampler:\n",
    "    def __init__(self):\n",
    "        self.key = jrand.PRNGKey(randint(0, 10**6))\n",
    "        \n",
    "    def normal(self, *shape):\n",
    "        self.key, subkey = jrand.split(self.key)\n",
    "        return jrand.normal(subkey, shape=tuple(shape))\n",
    "    \n",
    "    def uniform(self, *shape):\n",
    "        self.key, subkey = jrand.split(self.key)\n",
    "        return jrand.uniform(subkey, shape=tuple(shape))\n",
    "    \n",
    "dist_sampler = DistSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29509d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed70e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, nn, jit\n",
    "\n",
    "num_obs, num_ftrs = Obs.shape\n",
    "\n",
    "def xlayer(x, w, b):\n",
    "    # x has dims (#In,) and w has dims (#Out, #In) and b has dims (#Out,)\n",
    "    return jnp.dot(w, x) + b\n",
    "\n",
    "def compose_layers(activations, index=0):\n",
    "    afn = activations[index]\n",
    "    if len(activations) - 1 == index:\n",
    "        return lambda x, params : afn(xlayer(x, params[2*index], params[2*index+1]))\n",
    "    else:\n",
    "        nextfn = compose_layers(activations, index + 1)\n",
    "        return lambda x, params : nextfn(\n",
    "            afn(xlayer(x, params[2*index], params[2*index+1])),\n",
    "            params\n",
    "        )\n",
    "\n",
    "def make_nn_and_params(*layers, in_dim=None, sampler=None):\n",
    "    activations, dims = list(zip(*layers))\n",
    "    composed_layers = compose_layers(activations)\n",
    "    nn_func = lambda x, params : nn.softmax(composed_layers(x, params))\n",
    "    \n",
    "    def make_params():\n",
    "        param_list = []\n",
    "        prev_dim = in_dim\n",
    "        for d in dims:\n",
    "            param_list.append(sampler(d, prev_dim)) # Wi\n",
    "            param_list.append(sampler(d)) # Bi\n",
    "            prev_dim = d\n",
    "        return param_list\n",
    "\n",
    "    return nn_func, make_params\n",
    "\n",
    "neural_net, make_params = make_nn_and_params(\n",
    "    (jnp.tanh, 30),\n",
    "    (jnp.tanh, 50),\n",
    "    (jnp.tanh, 10),\n",
    "    in_dim = 784,\n",
    "    sampler = dist_sampler.normal\n",
    ")\n",
    "\n",
    "# def neural_net(x, params):\n",
    "#     w1, b1, w2, b2 = params #, w3, b3 = params\n",
    "#     t0 = tanh_layer(x, w1, b1)\n",
    "#     t1 = tanh_layer(t0, w2, b2)\n",
    "#     #t2 = leaky_relu_layer(t1, w3, b3)\n",
    "#     return nn.softmax(t1)\n",
    "\n",
    "# params = [\n",
    "#     dist_sampler.normal(3000, num_ftrs), # w1\n",
    "#     dist_sampler.normal(3000), # b1\n",
    "# #     dist_sampler.uniform(50, 784), # w2\n",
    "# #     dist_sampler.uniform(50), # b2\n",
    "#     dist_sampler.normal(10, 3000), # w3\n",
    "#     dist_sampler.normal(10), # b3\n",
    "# ]\n",
    "\n",
    "def cross_entropy(prediction, truth):\n",
    "    return -truth * jnp.log(prediction) - (1. - truth)*jnp.log(1. - prediction)\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    out = neural_net(x, params)\n",
    "    cross_entropy_vec = cross_entropy(out, y)\n",
    "    ce = jnp.sum(cross_entropy_vec)\n",
    "    return ce\n",
    "\n",
    "# GOTCHA\n",
    "# by default, only the first paramter of the input function\n",
    "# will be differentiated against\n",
    "loss_gradient = jit(grad(loss_fn))\n",
    "jit_nn = jit(neural_net)\n",
    "jit_loss = jit(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de6a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import vmap\n",
    "\n",
    "def make_loss_grad(params):\n",
    "    loss_grad = grad(loss_fn)\n",
    "    partial_loss_grad = partial(loss_grad, params)\n",
    "    vector_loss_grad = vmap(partial_loss_grad, (0, 0), 0)\n",
    "    return jit(vector_loss_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "752844d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def random_rows(n, *arrs):\n",
    "    idxs = sample(list(range(arrs[0].shape[0])), n)\n",
    "    return [\n",
    "        jnp.take(x, jnp.asarray(idxs), axis=0) for x in arrs\n",
    "    ]\n",
    "\n",
    "def sgd_minibatch(params, batch_size, learning_rate=0.01):\n",
    "    new_params = [jnp.copy(p) for p in params]\n",
    "    for _ in range(batch_size):\n",
    "        idx = randint(0, num_obs - 1)\n",
    "        x, y = Obs[idx], Resp[idx]\n",
    "        grads = loss_gradient(params, x, y)\n",
    "        new_params = [\n",
    "            param_vec - ((learning_rate / batch_size) * grad)\n",
    "            for param_vec, grad in zip(new_params, grads)\n",
    "        ]\n",
    "    return new_params\n",
    "\n",
    "def sgd_vector_minibatch(params, batch_size, learning_rate=0.01):\n",
    "    loss_grad = make_loss_grad(params)\n",
    "    batch_x, batch_y = random_rows(batch_size, Obs, Resp)\n",
    "    param_gradients = loss_grad(batch_x, batch_y)\n",
    "    average_grad_per_param = [\n",
    "        jnp.average(gradients, 0)\n",
    "        for gradients in param_gradients\n",
    "    ]\n",
    "    updated_params = [\n",
    "        param - (learning_rate * average_grad)\n",
    "        for param, average_grad in zip(params, average_grad_per_param)\n",
    "    ]\n",
    "    return updated_params\n",
    "\n",
    "def avg_training_loss(params, n=1000):\n",
    "    partial_loss = partial(jit_loss, params)\n",
    "    vector_loss = vmap(partial_loss, (0, 0), 0)\n",
    "    xs, ys = random_rows(n, Obs, Resp)\n",
    "    losses = vector_loss(xs, ys)\n",
    "    return jnp.average(losses, 0)\n",
    "    \n",
    "def avg_prediction_accuracy(params, n=1000):\n",
    "    check_guess = (\n",
    "        lambda x, y : jnp.argmax(jit_nn(x, params)) == jnp.argmax(y)\n",
    "    )\n",
    "    vector_nn = vmap(check_guess, (0,0), 0)\n",
    "    xs, ys = random_rows(n, TestObs, TestResp)\n",
    "    scores = vector_nn(xs, ys)\n",
    "    return jnp.average(scores, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d8666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100)\taverage loss:\t\t1.5602\n",
      "\taverage accuracy:\t91.10%\n",
      "\n",
      "(200)\taverage loss:\t\t1.5473\n",
      "\taverage accuracy:\t89.60%\n",
      "\n",
      "(300)\taverage loss:\t\t1.6171\n",
      "\taverage accuracy:\t91.30%\n",
      "\n",
      "(400)\taverage loss:\t\t1.5565\n",
      "\taverage accuracy:\t89.40%\n",
      "\n",
      "(500)\taverage loss:\t\t1.5793\n",
      "\taverage accuracy:\t90.50%\n",
      "\n",
      "(600)\taverage loss:\t\t1.5612\n",
      "\taverage accuracy:\t94.00%\n",
      "\n",
      "(700)\taverage loss:\t\t1.5776\n",
      "\taverage accuracy:\t91.20%\n",
      "\n",
      "(800)\taverage loss:\t\t1.5577\n",
      "\taverage accuracy:\t90.70%\n",
      "\n",
      "(900)\taverage loss:\t\t1.5253\n",
      "\taverage accuracy:\t90.30%\n",
      "\n",
      "(1000)\taverage loss:\t\t1.5317\n",
      "\taverage accuracy:\t91.20%\n",
      "\n",
      "(1100)\taverage loss:\t\t1.5566\n",
      "\taverage accuracy:\t91.40%\n",
      "\n",
      "(1200)\taverage loss:\t\t1.5342\n",
      "\taverage accuracy:\t92.00%\n",
      "\n",
      "(1300)\taverage loss:\t\t1.5443\n",
      "\taverage accuracy:\t91.30%\n",
      "\n",
      "(1400)\taverage loss:\t\t1.5386\n",
      "\taverage accuracy:\t93.10%\n",
      "\n",
      "(1500)\taverage loss:\t\t1.5417\n",
      "\taverage accuracy:\t92.30%\n",
      "\n",
      "(1600)\taverage loss:\t\t1.5423\n",
      "\taverage accuracy:\t91.30%\n",
      "\n",
      "(1700)\taverage loss:\t\t1.5205\n",
      "\taverage accuracy:\t91.90%\n",
      "\n",
      "(1800)\taverage loss:\t\t1.5299\n",
      "\taverage accuracy:\t94.00%\n",
      "\n",
      "(1900)\taverage loss:\t\t1.5363\n",
      "\taverage accuracy:\t92.40%\n",
      "\n",
      "(2000)\taverage loss:\t\t1.5440\n",
      "\taverage accuracy:\t92.80%\n",
      "\n",
      "(2100)\taverage loss:\t\t1.5278\n",
      "\taverage accuracy:\t92.60%\n",
      "\n",
      "(2200)\taverage loss:\t\t1.5141\n",
      "\taverage accuracy:\t92.40%\n",
      "\n",
      "(2300)\taverage loss:\t\t1.5294\n",
      "\taverage accuracy:\t92.80%\n",
      "\n",
      "(2400)\taverage loss:\t\t1.5113\n",
      "\taverage accuracy:\t92.20%\n",
      "\n",
      "(2500)\taverage loss:\t\t1.5277\n",
      "\taverage accuracy:\t91.60%\n",
      "\n",
      "(2600)\taverage loss:\t\t1.5325\n",
      "\taverage accuracy:\t92.60%\n",
      "\n",
      "(2700)\taverage loss:\t\t1.5419\n",
      "\taverage accuracy:\t92.00%\n",
      "\n",
      "(2800)\taverage loss:\t\t1.5102\n",
      "\taverage accuracy:\t92.40%\n",
      "\n",
      "(2900)\taverage loss:\t\t1.5265\n",
      "\taverage accuracy:\t91.90%\n",
      "\n",
      "(3000)\taverage loss:\t\t1.5352\n",
      "\taverage accuracy:\t92.20%\n",
      "\n",
      "(3100)\taverage loss:\t\t1.5074\n",
      "\taverage accuracy:\t93.00%\n",
      "\n",
      "(3200)\taverage loss:\t\t1.5342\n",
      "\taverage accuracy:\t92.20%\n",
      "\n",
      "(3300)\taverage loss:\t\t1.5593\n",
      "\taverage accuracy:\t93.90%\n",
      "\n",
      "(3400)\taverage loss:\t\t1.5384\n",
      "\taverage accuracy:\t92.50%\n",
      "\n",
      "(3500)\taverage loss:\t\t1.5186\n",
      "\taverage accuracy:\t92.20%\n",
      "\n",
      "(3600)\taverage loss:\t\t1.5216\n",
      "\taverage accuracy:\t91.20%\n",
      "\n",
      "(3700)\taverage loss:\t\t1.5004\n",
      "\taverage accuracy:\t93.80%\n",
      "\n",
      "(3800)\taverage loss:\t\t1.5147\n",
      "\taverage accuracy:\t92.90%\n",
      "\n",
      "(3900)\taverage loss:\t\t1.5059\n",
      "\taverage accuracy:\t92.50%\n",
      "\n",
      "(4000)\taverage loss:\t\t1.4971\n",
      "\taverage accuracy:\t93.20%\n",
      "\n",
      "(4100)\taverage loss:\t\t1.5231\n",
      "\taverage accuracy:\t92.80%\n",
      "\n",
      "(4200)\taverage loss:\t\t1.5411\n",
      "\taverage accuracy:\t93.00%\n",
      "\n",
      "(4300)\taverage loss:\t\t1.5055\n",
      "\taverage accuracy:\t93.30%\n",
      "\n",
      "(4400)\taverage loss:\t\t1.4788\n",
      "\taverage accuracy:\t92.60%\n",
      "\n",
      "(4500)\taverage loss:\t\t1.5203\n",
      "\taverage accuracy:\t92.10%\n",
      "\n",
      "(4600)\taverage loss:\t\t1.4914\n",
      "\taverage accuracy:\t93.20%\n",
      "\n",
      "(4700)\taverage loss:\t\t1.4933\n",
      "\taverage accuracy:\t92.50%\n",
      "\n",
      "(4800)\taverage loss:\t\t1.4949\n",
      "\taverage accuracy:\t93.50%\n",
      "\n",
      "(4900)\taverage loss:\t\t1.5127\n",
      "\taverage accuracy:\t93.70%\n",
      "\n",
      "(5000)\taverage loss:\t\t1.5099\n",
      "\taverage accuracy:\t92.50%\n",
      "\n",
      "(5100)\taverage loss:\t\t1.5221\n",
      "\taverage accuracy:\t92.70%\n",
      "\n",
      "(5200)\taverage loss:\t\t1.5203\n",
      "\taverage accuracy:\t93.40%\n",
      "\n",
      "(5300)\taverage loss:\t\t1.5207\n",
      "\taverage accuracy:\t92.40%\n",
      "\n",
      "(5400)\taverage loss:\t\t1.4833\n",
      "\taverage accuracy:\t92.50%\n",
      "\n",
      "(5500)\taverage loss:\t\t1.4880\n",
      "\taverage accuracy:\t93.40%\n",
      "\n",
      "(5600)\taverage loss:\t\t1.4967\n",
      "\taverage accuracy:\t93.40%\n",
      "\n",
      "(5700)\taverage loss:\t\t1.5026\n",
      "\taverage accuracy:\t93.40%\n",
      "\n",
      "(5800)\taverage loss:\t\t1.5252\n",
      "\taverage accuracy:\t94.60%\n",
      "\n",
      "(5900)\taverage loss:\t\t1.5143\n",
      "\taverage accuracy:\t93.00%\n",
      "\n",
      "(6000)\taverage loss:\t\t1.4895\n",
      "\taverage accuracy:\t93.40%\n",
      "\n",
      "(6100)\taverage loss:\t\t1.5254\n",
      "\taverage accuracy:\t93.70%\n",
      "\n",
      "(6200)\taverage loss:\t\t1.5226\n",
      "\taverage accuracy:\t93.60%\n",
      "\n",
      "(6300)\taverage loss:\t\t1.5062\n",
      "\taverage accuracy:\t93.90%\n",
      "\n",
      "(6400)\taverage loss:\t\t1.4888\n",
      "\taverage accuracy:\t94.50%\n",
      "\n",
      "(6500)\taverage loss:\t\t1.4752\n",
      "\taverage accuracy:\t93.40%\n",
      "\n",
      "(6600)\taverage loss:\t\t1.5269\n",
      "\taverage accuracy:\t92.80%\n",
      "\n",
      "(6700)\taverage loss:\t\t1.4685\n",
      "\taverage accuracy:\t92.10%\n",
      "\n",
      "(6800)\taverage loss:\t\t1.5039\n",
      "\taverage accuracy:\t94.70%\n",
      "\n",
      "(6900)\taverage loss:\t\t1.5081\n",
      "\taverage accuracy:\t93.70%\n",
      "\n",
      "(7000)\taverage loss:\t\t1.5024\n",
      "\taverage accuracy:\t94.00%\n",
      "\n",
      "(7100)\taverage loss:\t\t1.5415\n",
      "\taverage accuracy:\t92.60%\n",
      "\n",
      "(7200)\taverage loss:\t\t1.4820\n",
      "\taverage accuracy:\t94.30%\n",
      "\n",
      "(7300)\taverage loss:\t\t1.5005\n",
      "\taverage accuracy:\t93.30%\n",
      "\n",
      "(7400)\taverage loss:\t\t1.4834\n",
      "\taverage accuracy:\t93.30%\n",
      "\n",
      "(7500)\taverage loss:\t\t1.4912\n",
      "\taverage accuracy:\t94.70%\n",
      "\n",
      "(7600)\taverage loss:\t\t1.4945\n",
      "\taverage accuracy:\t93.10%\n",
      "\n",
      "(7700)\taverage loss:\t\t1.5112\n",
      "\taverage accuracy:\t92.50%\n",
      "\n",
      "(7800)\taverage loss:\t\t1.4736\n",
      "\taverage accuracy:\t93.70%\n",
      "\n",
      "(7900)\taverage loss:\t\t1.4976\n",
      "\taverage accuracy:\t94.50%\n",
      "\n",
      "(8000)\taverage loss:\t\t1.5072\n",
      "\taverage accuracy:\t93.00%\n",
      "\n",
      "(8100)\taverage loss:\t\t1.4874\n",
      "\taverage accuracy:\t93.30%\n",
      "\n",
      "(8200)\taverage loss:\t\t1.4874\n",
      "\taverage accuracy:\t93.50%\n",
      "\n",
      "(8300)\taverage loss:\t\t1.4841\n",
      "\taverage accuracy:\t93.70%\n",
      "\n",
      "(8400)\taverage loss:\t\t1.4939\n",
      "\taverage accuracy:\t93.10%\n",
      "\n",
      "(8500)\taverage loss:\t\t1.4664\n",
      "\taverage accuracy:\t93.90%\n",
      "\n",
      "(8600)\taverage loss:\t\t1.4896\n",
      "\taverage accuracy:\t93.70%\n",
      "\n",
      "(8700)\taverage loss:\t\t1.4720\n",
      "\taverage accuracy:\t95.90%\n",
      "\n",
      "(8800)\taverage loss:\t\t1.4721\n",
      "\taverage accuracy:\t94.50%\n",
      "\n",
      "(8900)\taverage loss:\t\t1.4811\n",
      "\taverage accuracy:\t93.80%\n",
      "\n",
      "(9000)\taverage loss:\t\t1.5271\n",
      "\taverage accuracy:\t93.30%\n",
      "\n",
      "(9100)\taverage loss:\t\t1.4718\n",
      "\taverage accuracy:\t92.90%\n",
      "\n",
      "(9200)\taverage loss:\t\t1.4982\n",
      "\taverage accuracy:\t93.60%\n",
      "\n",
      "(9300)\taverage loss:\t\t1.4671\n",
      "\taverage accuracy:\t92.00%\n",
      "\n",
      "(9400)\taverage loss:\t\t1.5001\n",
      "\taverage accuracy:\t93.90%\n",
      "\n",
      "(9500)\taverage loss:\t\t1.4758\n",
      "\taverage accuracy:\t94.50%\n",
      "\n",
      "(9600)\taverage loss:\t\t1.5366\n",
      "\taverage accuracy:\t93.20%\n",
      "\n",
      "(9700)\taverage loss:\t\t1.4535\n",
      "\taverage accuracy:\t94.20%\n",
      "\n",
      "(9800)\taverage loss:\t\t1.4823\n",
      "\taverage accuracy:\t94.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, epoch_num in enumerate(range(10_000)):\n",
    "    params = sgd_vector_minibatch(params, 100, .5)\n",
    "    #params = sgd_minibatch(params, 30, .5)\n",
    "    if (i+1) % 100 == 0:\n",
    "        loss = avg_training_loss(params)\n",
    "        print(f'({i+1})\\taverage loss:\\t\\t{loss:.4f}')\n",
    "        acc = avg_prediction_accuracy(params)\n",
    "        print(f'\\taverage accuracy:\\t{(acc * 100):.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616ca96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# class Clock():\n",
    "#     def __init__(self):\n",
    "#         self.start_time = time.time()\n",
    "        \n",
    "#     def start(self):\n",
    "#         self.start_time = time.time()\n",
    "    \n",
    "#     def stop(self):\n",
    "#         return time.time() - self.start_time\n",
    "    \n",
    "# clock = Clock()\n",
    "\n",
    "# clock.start()\n",
    "# for i, epoch_num in enumerate(range(100)):\n",
    "#     params = sgd_minibatch(params, 50, .5)\n",
    "#     if (i+1) % 20 == 0:\n",
    "#         print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "# print(f'minibatch time = {clock.stop()}')\n",
    "\n",
    "# clock.start()\n",
    "# for i, epoch_num in enumerate(range(100)):\n",
    "#     params = sgd_vector_minibatch(params, 50, .5)\n",
    "#     if (i+1) % 20 == 0:\n",
    "#         print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "# print(f'vector_minibatch time = {clock.stop()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b3f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nns",
   "language": "python",
   "name": "nns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68dacc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0),\n",
       " StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698a883",
   "metadata": {},
   "source": [
    "# *Solving MNIST with JAX*\n",
    "\n",
    "JAX is a cool library. Among other things, it:\n",
    "- can JIT compile code for a CPU/GPU/TPU/etc...\n",
    "- makes parallel execution easy, even on separate devices\n",
    "- can transform a scalar-valued function into one that computes its gradient\n",
    "\n",
    "What better way to explore this library than with a neural net?\n",
    "\n",
    "Special thanks to [You Don't Know JAX](https://colinraffel.com/blog/you-don-t-know-jax.html) and [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f65f4",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "We can load the MNIST dataset into a pandas dataframe via OpenML and sklearn's OpenML interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b50c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_mnist():\n",
    "    pickle_path = '../data/mnist/data.pkl'\n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    mnist = fetch_openml(name='mnist_784', version=1, parser='auto')\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "    return mnist\n",
    "\n",
    "mnist = load_mnist()\n",
    "all_features, all_targets = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861baa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image\n",
      "\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "         @@@      @         \n",
      "        @@@@@    @@         \n",
      "        @@      @@          \n",
      "        @@      @@          \n",
      "        @@      @@          \n",
      "         @@     @@          \n",
      "         @@     @@          \n",
      "         @@@    @@          \n",
      "          @@@   @@          \n",
      "          @@@   @@          \n",
      "           @@@  @@          \n",
      "            @@@@@@          \n",
      "             @@@@@          \n",
      "              @@@           \n",
      "               @@           \n",
      "              @@@           \n",
      "              @@            \n",
      "              @@            \n",
      "              @@            \n",
      "              @             \n",
      "                            \n",
      "                            \n",
      "\n",
      "Sample answer: 4\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle, randint\n",
    "from jax import numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Split the data into train and test segments, then format it as JAX matrices\n",
    "\"\"\"\n",
    "bool_vec = [i < 60_000 for i in range(len(all_targets))]\n",
    "shuffle(bool_vec)\n",
    "split_df = lambda df : (\n",
    "    df[pd.Series(bool_vec).values],\n",
    "    df[pd.Series([not b for b in bool_vec]).values]\n",
    ")\n",
    "\n",
    "train_features, test_features = split_df(all_features)\n",
    "train_targets, test_targets = split_df(all_targets)\n",
    "\n",
    "format_jnp = lambda *dfs : tuple([jnp.asarray(df.to_numpy(), dtype='float32') for df in dfs])\n",
    "\n",
    "ftr_train, ftr_test, tgt_train, tgt_test = format_jnp(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    pd.get_dummies(train_targets),\n",
    "    pd.get_dummies(test_targets)\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Spot check: print a rough sketch of a sample number, along with the expected answer.\n",
    "\"\"\"\n",
    "print('Sample image\\n')\n",
    "\n",
    "idx = randint(0, ftr_train.shape[0])\n",
    "x, y = ftr_train[idx], tgt_train[idx]\n",
    "\n",
    "img = jnp.reshape(x, (28,28))\n",
    "for row in img:\n",
    "    print(''.join(['@' if pix > 100 else ' ' for pix in row]))\n",
    "    \n",
    "print(f'\\nSample answer: {jnp.argmax(y)}')\n",
    "\n",
    "\"\"\"\n",
    "Normalize the features\n",
    "\"\"\"\n",
    "normalize = lambda ftr_df : ftr_df / jnp.linalg.norm(ftr_df, axis=1, keepdims=True)\n",
    "ftr_train, ftr_test = normalize(ftr_train), normalize(ftr_test)\n",
    "Obs, Resp = ftr_train, tgt_train\n",
    "TestObs, TestResp = ftr_test, tgt_test\n",
    "# quick sanity check\n",
    "assert all(jnp.isclose(jnp.linalg.norm(Obs, axis=1), jnp.full(Obs.shape[0], 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a31de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed: 784137\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from jax import random as jrand\n",
    "\n",
    "class DistSampler:\n",
    "    def __init__(self, seed=None):\n",
    "        seed = seed or randint(0, 10**6)\n",
    "        self.key = jrand.PRNGKey(seed)\n",
    "        print('Using seed:', seed)\n",
    "        \n",
    "    def normal(self, *shape):\n",
    "        self.key, _ = jrand.split(self.key)\n",
    "        return jrand.normal(self.key, shape=tuple(shape))\n",
    "    \n",
    "    def choice(self, n, k):\n",
    "        self.key, _ = jrand.split(self.key)\n",
    "        return list(map(int, jrand.choice(self.key, n, (k,), replace=False)))\n",
    "    \n",
    "    def random_rows(self, n, *arrs):\n",
    "        idxs = self.choice(arrs[0].shape[0], n) # assume arrays have same outer dim\n",
    "        return [jnp.take(x, jnp.asarray(idxs), axis=0) for x in arrs]\n",
    "    \n",
    "    def random_idxs(self, n, upper, k):\n",
    "        return [self.choice(upper, k) for _ in range(n)]\n",
    "    \n",
    "    def random_rows_idxs(self, idxs, *arrs):\n",
    "        return [jnp.take(x, jnp.asarray(idxs), axis=0) for x in arrs]\n",
    "    \n",
    "    def shuffle(self, arr):\n",
    "        self.key, _ = jrand.split(self.key)\n",
    "        return jrand.permutation(self.key, arr)\n",
    "    \n",
    "seed = None\n",
    "dist_sampler = DistSampler(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29509d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed70e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, nn, jit\n",
    "\n",
    "num_obs, num_ftrs = Obs.shape\n",
    "\n",
    "def xlayer(x, w, b):\n",
    "    # x has dims (#In,) and w has dims (#Out, #In) and b has dims (#Out,)\n",
    "    return jnp.dot(w, x) + b\n",
    "\n",
    "def compose_layers(activations, index=0):\n",
    "    afn = activations[index]\n",
    "    if len(activations) - 1 == index:\n",
    "        return lambda x, params : afn(xlayer(x, params[2*index], params[2*index+1]))\n",
    "    else:\n",
    "        nextfn = compose_layers(activations, index + 1)\n",
    "        return lambda x, params : nextfn(\n",
    "            afn(xlayer(x, params[2*index], params[2*index+1])),\n",
    "            params\n",
    "        )\n",
    "\n",
    "def make_nn_and_params(*layers, in_dim=None, sampler=None):\n",
    "    activations, dims = list(zip(*layers))\n",
    "    composed_layers = compose_layers(activations)\n",
    "    nn_func = lambda x, params : nn.softmax(composed_layers(x, params))\n",
    "    \n",
    "    def make_params():\n",
    "        param_list = []\n",
    "        prev_dim = in_dim\n",
    "        for d in dims:\n",
    "            param_list.append(sampler(d, prev_dim)) # Wi\n",
    "            param_list.append(sampler(d)) # Bi\n",
    "            prev_dim = d\n",
    "        return param_list\n",
    "\n",
    "    return nn_func, make_params\n",
    "\n",
    "neural_net, make_params = make_nn_and_params(\n",
    "    (jnp.tanh, 30),\n",
    "    (jnp.tanh, 50),\n",
    "    (jnp.tanh, 10),\n",
    "    in_dim = 784,\n",
    "    sampler = dist_sampler.normal\n",
    ")\n",
    "\n",
    "# def neural_net(x, params):\n",
    "#     w1, b1, w2, b2 = params #, w3, b3 = params\n",
    "#     t0 = tanh_layer(x, w1, b1)\n",
    "#     t1 = tanh_layer(t0, w2, b2)\n",
    "#     #t2 = leaky_relu_layer(t1, w3, b3)\n",
    "#     return nn.softmax(t1)\n",
    "\n",
    "# params = [\n",
    "#     dist_sampler.normal(3000, num_ftrs), # w1\n",
    "#     dist_sampler.normal(3000), # b1\n",
    "# #     dist_sampler.uniform(50, 784), # w2\n",
    "# #     dist_sampler.uniform(50), # b2\n",
    "#     dist_sampler.normal(10, 3000), # w3\n",
    "#     dist_sampler.normal(10), # b3\n",
    "# ]\n",
    "\n",
    "def cross_entropy(prediction, truth):\n",
    "    return -truth * jnp.log(prediction) - (1. - truth)*jnp.log(1. - prediction)\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    out = neural_net(x, params)\n",
    "    cross_entropy_vec = cross_entropy(out, y)\n",
    "    ce = jnp.sum(cross_entropy_vec)\n",
    "    return ce\n",
    "\n",
    "# GOTCHA\n",
    "# by default, only the first paramter of the input function\n",
    "# will be differentiated against\n",
    "loss_gradient = jit(grad(loss_fn))\n",
    "jit_nn = jit(neural_net)\n",
    "jit_loss = jit(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de6a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import vmap\n",
    "\n",
    "def make_loss_grad_nojit(params):\n",
    "    loss_grad = grad(loss_fn)\n",
    "    partial_loss_grad = partial(loss_grad, params)\n",
    "    vector_loss_grad = vmap(partial_loss_grad, (0, 0), 0)\n",
    "    return vector_loss_grad\n",
    "\n",
    "def make_loss_grad(params):\n",
    "    return jit(make_loss_grad_nojit(params))\n",
    "\n",
    "def make_loss_grad_with_params_nojit():\n",
    "    loss_grad = grad(loss_fn)\n",
    "    return vmap(loss_grad, (None, 0, 0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752844d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def random_rows(n, *arrs):\n",
    "    idxs = sample(list(range(arrs[0].shape[0])), n)\n",
    "    return [\n",
    "        jnp.take(x, jnp.asarray(idxs), axis=0) for x in arrs\n",
    "    ]\n",
    "\n",
    "def sgd_minibatch(params, batch_size, learning_rate=0.01):\n",
    "    new_params = [jnp.copy(p) for p in params]\n",
    "    for _ in range(batch_size):\n",
    "        idx = randint(0, num_obs - 1)\n",
    "        x, y = Obs[idx], Resp[idx]\n",
    "        grads = loss_gradient(params, x, y)\n",
    "        new_params = [\n",
    "            param_vec - ((learning_rate / batch_size) * grad)\n",
    "            for param_vec, grad in zip(new_params, grads)\n",
    "        ]\n",
    "    return new_params\n",
    "\n",
    "def sgd_vector_minibatch(params, batch_size, learning_rate=0.01):\n",
    "    loss_grad = make_loss_grad(params)\n",
    "    batch_x, batch_y = random_rows(batch_size, Obs, Resp)\n",
    "    param_gradients = loss_grad(batch_x, batch_y)\n",
    "    average_grad_per_param = [\n",
    "        jnp.average(gradients, 0)\n",
    "        for gradients in param_gradients\n",
    "    ]\n",
    "    updated_params = [\n",
    "        param - (learning_rate * average_grad)\n",
    "        for param, average_grad in zip(params, average_grad_per_param)\n",
    "    ]\n",
    "    return updated_params\n",
    "\n",
    "def avg_training_loss(params, n=1000):\n",
    "    partial_loss = partial(jit_loss, params)\n",
    "    vector_loss = vmap(partial_loss, (0, 0), 0)\n",
    "    xs, ys = random_rows(n, Obs, Resp)\n",
    "    losses = vector_loss(xs, ys)\n",
    "    return jnp.average(losses, 0)\n",
    "    \n",
    "def avg_prediction_accuracy(params, n=1000):\n",
    "    check_guess = (\n",
    "        lambda x, y : jnp.argmax(jit_nn(x, params)) == jnp.argmax(y)\n",
    "    )\n",
    "    vector_nn = vmap(check_guess, (0,0), 0)\n",
    "    xs, ys = random_rows(n, TestObs, TestResp)\n",
    "    scores = vector_nn(xs, ys)\n",
    "    return jnp.average(scores, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d8666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad_fn = make_loss_grad_with_params_nojit()\n",
    "\n",
    "@jit\n",
    "def batch_train(params, x, y, learning_rate):\n",
    "    param_gradients = loss_grad_fn(params, x, y)\n",
    "    average_grad_per_param = [\n",
    "        jnp.average(gradients, 0)\n",
    "        for gradients in param_gradients\n",
    "    ]\n",
    "    updated_params = [\n",
    "        param - (learning_rate * average_grad)\n",
    "        for param, average_grad in zip(params, average_grad_per_param)\n",
    "    ]\n",
    "    return updated_params\n",
    "\n",
    "#@partial(jit, static_argnums=(0,))\n",
    "def fori_batch_train(i, argtup):\n",
    "    params, batch_indexes, learning_rate = argtup\n",
    "    idxs = batch_indexes[i]\n",
    "    batch_x, batch_y = dist_sampler.random_rows_idxs(idxs, Obs, Resp)\n",
    "    updated_params = batch_train(params, batch_x, batch_y, learning_rate)\n",
    "    return (updated_params, batch_indexes, learning_rate)\n",
    "\n",
    "#@partial(jit, static_argnums=(1,))\n",
    "def recursive_batch_train(params, iterations, batch_size, learning_rate):\n",
    "    batch_indexes = jnp.asarray(dist_sampler.random_idxs(iterations, Obs.shape[0], batch_size))\n",
    "    params, _, __ = jax.lax.fori_loop(\n",
    "        0,\n",
    "        len(batch_indexes),\n",
    "        fori_batch_train,\n",
    "        (params, batch_indexes, learning_rate)\n",
    "    )\n",
    "    return params\n",
    "#     # jittable if\n",
    "#     return jnp.where(\n",
    "#         iterations > 0,\n",
    "#         recursive_batch_train(updated_params, batch_size, learning_rate, iterations - 1),\n",
    "#         updated_params\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e290054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, epoch_num in enumerate(range(10_000)):\n",
    "#     params = sgd_vector_minibatch(params, 100, .5)\n",
    "#     #params = sgd_minibatch(params, 30, .5)\n",
    "#     if (i+1) % 100 == 0:\n",
    "#         loss = avg_training_loss(params)\n",
    "#         print(f'({i+1})\\taverage loss:\\t\\t{loss:.4f}')\n",
    "#         acc = avg_prediction_accuracy(params)\n",
    "#         print(f'\\taverage accuracy:\\t{(acc * 100):.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a399fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Clock():\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def stop(self):\n",
    "        return time.time() - self.start_time\n",
    "    \n",
    "clock = Clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6616ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1609eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jit_vector_minibatch time = 2.761857748031616\n",
      "avg prediction accuracy = 47.40000534057617\n",
      "jit_vector_minibatch time = 0.7563719749450684\n",
      "avg prediction accuracy = 54.599998474121094\n",
      "jit_vector_minibatch time = 0.6820828914642334\n",
      "avg prediction accuracy = 62.0\n",
      "jit_vector_minibatch time = 0.7351155281066895\n",
      "avg prediction accuracy = 64.40000915527344\n",
      "jit_vector_minibatch time = 0.7291135787963867\n",
      "avg prediction accuracy = 67.0999984741211\n"
     ]
    }
   ],
   "source": [
    "#jax.profiler.start_trace('../profiles', create_perfetto_trace=True)\n",
    "\n",
    "batch_size = 50\n",
    "iterations = 100\n",
    "learning_rate = 0.5\n",
    "\n",
    "# clock.start()\n",
    "# for i, epoch_num in enumerate(range(iterations)):\n",
    "#     params = sgd_minibatch(params, batch_size, learning_rate)\n",
    "# #     if (i+1) % 20 == 0:\n",
    "# #         print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "# print(f'minibatch time = {clock.stop()}')\n",
    "\n",
    "# clock.start()\n",
    "# for i, epoch_num in enumerate(range(iterations)):\n",
    "#     params = sgd_vector_minibatch(params, batch_size, learning_rate)\n",
    "# #     if (i+1) % 20 == 0:\n",
    "# #         print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "# print(f'vector_minibatch time = {clock.stop()}')\n",
    "\n",
    "jax.profiler.start_trace('../profiles', create_perfetto_trace=True)\n",
    "\n",
    "for i in range(5):\n",
    "    clock.start()\n",
    "    params = recursive_batch_train(params, iterations, batch_size, learning_rate)\n",
    "    #print(f'({i+1}) average loss: {avg_training_loss(params)}')\n",
    "    print(f'jit_vector_minibatch time = {clock.stop()}')\n",
    "    print(f'avg prediction accuracy = {100 * avg_prediction_accuracy(params)}')\n",
    "\n",
    "jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b3f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nns",
   "language": "python",
   "name": "nns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4698a883",
   "metadata": {},
   "source": [
    "# *Solving MNIST with JAX*\n",
    "\n",
    "JAX is a cool library. Among other things, it:\n",
    "- can JIT compile code for a CPU/GPU/TPU/etc...\n",
    "- makes parallel execution easy, even on separate devices\n",
    "- can transform a scalar-valued function into one that computes its gradient\n",
    "\n",
    "What better way to explore this library than with a neural net?\n",
    "\n",
    "Special thanks to [You Don't Know JAX](https://colinraffel.com/blog/you-don-t-know-jax.html) and [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592cabc",
   "metadata": {},
   "source": [
    "### First, let's make sure we're able to use the GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c90a2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0),\n",
       " StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f65f4",
   "metadata": {},
   "source": [
    "## Downloading the MNIST dataset\n",
    "\n",
    "The MNIST dataset can be downloaded into a pandas dataframe via sklearn's OpenML interface. In order to save time in future runs, the pandas dataframe is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b50c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_mnist():\n",
    "    pickle_path = '../data/mnist/data.pkl'\n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    mnist = fetch_openml(name='mnist_784', version=1, parser='auto')\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "    return mnist\n",
    "\n",
    "mnist = load_mnist()\n",
    "all_features, all_targets = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31aff85",
   "metadata": {},
   "source": [
    "## Note: Randomness in JAX\n",
    "\n",
    "JAX offers a randomness utility very similar to that of numpy. The major difference is that you need to explicitly provide a seed. This is more tedious, but the upside is that you have more control over the process. For example if you use the same seed, you should end up with the same results.\n",
    "\n",
    "All randomness in this notebook is derived from the same seed - so, if you choose to run it (with the provdied key) you should get the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f113cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed: 298504\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from jax import random as jrand\n",
    "\n",
    "class DistSampler:\n",
    "    def __init__(self, seed=None):\n",
    "        seed = seed or randint(0, 10**6)\n",
    "        self.key = jrand.PRNGKey(seed)\n",
    "        print('Using seed:', seed)\n",
    "        \n",
    "    def normal(self, *shape):\n",
    "        self.key, _ = jrand.split(self.key)\n",
    "        return jrand.normal(self.key, shape=tuple(shape))\n",
    "    \n",
    "    def choice(self, n, k):\n",
    "        self.key, _ = jrand.split(self.key)\n",
    "        return list(map(int, jrand.choice(self.key, n, (k,), replace=False)))\n",
    "    \n",
    "    def random_rows(self, n, *arrs):\n",
    "        idxs = self.choice(arrs[0].shape[0], n) # assume arrays have same outer dim\n",
    "        return [jnp.take(x, jnp.asarray(idxs), axis=0) for x in arrs]\n",
    "    \n",
    "    def shuffle(self, arr):\n",
    "        self.key, _ = jrand.split(self.key)\n",
    "        return jrand.permutation(self.key, arr)\n",
    "    \n",
    "seed = 298504\n",
    "dist_sampler = DistSampler(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8dbcd",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Now that the dataset is available, it needs to be split into train and test sets and the feature vectors need to be normalized. For my own sanity, I also performed a quick spot check on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861baa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image:\n",
      "\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "             @@@            \n",
      "           @@@@@@@          \n",
      "          @@@@@@@@@@@@@     \n",
      "         @@@@@@  @@@@@@@    \n",
      "         @@@@      @@@@@    \n",
      "         @@@        @@@@    \n",
      "         @@@       @@@@     \n",
      "         @@@     @@@@@@     \n",
      "         @@@   @@@@@@       \n",
      "          @@  @@@@@@        \n",
      "          @@@@@@@@@         \n",
      "          @@@@@@@@          \n",
      "          @@@@@@            \n",
      "         @@@@@@@            \n",
      "        @@@@@@@@            \n",
      "        @@@@@ @@@           \n",
      "        @@@@@@@@@@          \n",
      "        @@@@@@@@@@          \n",
      "         @@@@@@@@           \n",
      "          @@@@@@            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "\n",
      "Sample answer: 8\n"
     ]
    }
   ],
   "source": [
    "from jax import numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Split the data into train and test segments, then format it as JAX matrices\n",
    "\"\"\"\n",
    "bool_vec = [i < 60_000 for i in range(len(all_targets))]\n",
    "bool_vec = [bool(x) for x in dist_sampler.shuffle(jnp.asarray(bool_vec))]\n",
    "split_df = lambda df : (\n",
    "    df[pd.Series(bool_vec).values],\n",
    "    df[pd.Series([not b for b in bool_vec]).values]\n",
    ")\n",
    "\n",
    "train_features, test_features = split_df(all_features)\n",
    "train_targets, test_targets = split_df(all_targets)\n",
    "\n",
    "format_jnp = lambda *dfs : tuple([jnp.asarray(df.to_numpy(), dtype='float32') for df in dfs])\n",
    "\n",
    "ftr_train, ftr_test, tgt_train, tgt_test = format_jnp(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    pd.get_dummies(train_targets),\n",
    "    pd.get_dummies(test_targets)\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Spot check: print a rough sketch of a sample number, along with the expected answer.\n",
    "\"\"\"\n",
    "print('Sample image:\\n')\n",
    "\n",
    "idx = dist_sampler.choice(ftr_train.shape[0], 1)[0]\n",
    "x, y = ftr_train[idx], tgt_train[idx]\n",
    "\n",
    "img = jnp.reshape(x, (28,28))\n",
    "for row in img:\n",
    "    print(''.join(['@' if pix > 100 else ' ' for pix in row]))\n",
    "    \n",
    "print(f'\\nSample answer: {jnp.argmax(y)}')\n",
    "\n",
    "\"\"\"\n",
    "Normalize the features\n",
    "\"\"\"\n",
    "normalize = lambda ftr_df : ftr_df / jnp.linalg.norm(ftr_df, axis=1, keepdims=True)\n",
    "ftr_train, ftr_test = normalize(ftr_train), normalize(ftr_test)\n",
    "Obs, Resp = ftr_train, tgt_train\n",
    "TestObs, TestResp = ftr_test, tgt_test\n",
    "# quick sanity check\n",
    "assert all(jnp.isclose(jnp.linalg.norm(Obs, axis=1), jnp.full(Obs.shape[0], 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae116e",
   "metadata": {},
   "source": [
    "## Constructing the Neural Network\n",
    "\n",
    "To leverage the full power of JAX (it's JIT-compiler in particular), it's important to make sure our functions are \"pure\", e.g. stateless. Not doing so can result in weird errors, which the JAX docs do a good job of touching on. The `compose_layers` function makes it easy to assemble the core of the neural net in an expressive manner.\n",
    "\n",
    "Note that the parameters are an explicit parameter of the neural network, rather than being \"baked in\". Personally, this is one of the things I like about working with JAX. The structure of the network (it's dimensions and activation functions) are clearly separated from the parameter values that are being learned, making the process of training more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed70e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import nn\n",
    "\n",
    "def cross_entropy(prediction, truth):\n",
    "    return \n",
    "\n",
    "\"\"\"\n",
    "Given a list of activation functions [A0, A1, ..., An], construct a function that takes\n",
    "an input X and a list of parameters [W0, B0, W1, B1, ..., Wn, Bn] and returns an output.\n",
    "\n",
    "The function will essentially have the structure:\n",
    "\n",
    "fn(x, [W0, B0, ..., Wn, Bn]):\n",
    "    return An(An-1(...(A0(X * W0 + B0)...) * Wn-1 + Bn-1) * Wn + Bn)\n",
    "\"\"\"\n",
    "def compose_layers(activations, index=0):\n",
    "    afn = activations[index]\n",
    "    wi = 2*index\n",
    "    bi = 2*index+1\n",
    "    \n",
    "    # x has dims (#In,)\n",
    "    # params[wi] has dims (#Out, #In)\n",
    "    # params[bi] has dims (#Out,)\n",
    "    layer_fn = lambda x, params : afn(jnp.dot(params[wi], x) + params[bi])\n",
    "    \n",
    "    if len(activations) - 1 == index:\n",
    "        return layer_fn\n",
    "    else:\n",
    "        nextfn = compose_layers(activations, index + 1)\n",
    "        return lambda x, params : nextfn(layer_fn(x, params), params)\n",
    "\n",
    "\"\"\"\n",
    "Function to construct the neural net and randomly initialize its parameters\n",
    "\"\"\"\n",
    "def make_nn_and_params(*layers, in_dim=None, sampler=None):\n",
    "    activations, dims = list(zip(*layers))\n",
    "    \n",
    "    # create the neural layer structure\n",
    "    composed_layers = compose_layers(activations)\n",
    "    \n",
    "    # wrap the final output in a softmax\n",
    "    nn_func = lambda x, params : nn.softmax(composed_layers(x, params))\n",
    "\n",
    "    # sample params from a normal distribution given the dimensions\n",
    "    param_list = []\n",
    "    prev_dim = in_dim\n",
    "    for d in dims:\n",
    "        param_list.append(sampler(d, prev_dim)) # Wi\n",
    "        param_list.append(sampler(d)) # Bi\n",
    "        prev_dim = d\n",
    "\n",
    "    return nn_func, param_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define a three-layer neural net of widths 50, 100 and 10,\n",
    "using the tanh function as an activation in each layer\n",
    "\"\"\"\n",
    "num_obs, num_ftrs = Obs.shape\n",
    "neural_net, neural_net_params = make_nn_and_params(\n",
    "    (jnp.tanh, 50),\n",
    "    (jnp.tanh, 100),\n",
    "    (jnp.tanh, 10),\n",
    "    in_dim = num_ftrs,\n",
    "    sampler = dist_sampler.normal\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Define the cross entropy loss as a function of the\n",
    "neural net parameters, an input and the expected outpt\n",
    "\n",
    "Note: by default, jax.grad will only differentiate \n",
    "      the first parameter of this function\n",
    "\"\"\"\n",
    "def loss_fn(params, x, y):\n",
    "    # define cross-entropy\n",
    "    cross_entropy = lambda prediction, truth : jnp.sum(\n",
    "        -truth * jnp.log(prediction) - (1. - truth) * jnp.log(1. - prediction)\n",
    "    )\n",
    "    # pass input and params through neural network\n",
    "    out = neural_net(x, params)\n",
    "    # compute the output\n",
    "    return cross_entropy(out, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fa98a",
   "metadata": {},
   "source": [
    "## Defining the training process\n",
    "\n",
    "Training a neural network is a simple (in this case) matter of gradient descent. That is, given a loss function, compute the average gradient G over a batch of inputs and then subtract G from the parameters (this is mini-batch stochastic gradient descent).\n",
    "\n",
    "\n",
    "Using JAX's function transformation utilities, the training process can be implemented at nearly the same level of abstraction as the description above; the compiler should ensure that such abstraction doesn't come at the cost of performance.\n",
    "\n",
    "\n",
    "The `make_loss_grad` function is where the JAX magic happens:\n",
    "1. `jax.grad` is applied to the loss function, which returns a function that takes the same input as the loss function, but returns its gradient instead.\n",
    "2. `functools.partial` is then used to \"embed\" the neural net parameters inside of the gradient function so that our gradient function will have the form `f : (input, expected_result) -> gradient`; this signature makes step 3 more straightforward...\n",
    "3. `jax.vmap` is applied to parallelize the gradient function, for the sake of performance\n",
    "4. `jax.jit` is applied so that the function can be jit-compiled, again for the sake of performance\n",
    "\n",
    "\n",
    "The `sgd_vector_minibatch` function then implments the gradient descent process, taking in the neural net parameters as input and returning a \"better\" set of parameters.\n",
    "\n",
    "\n",
    "Note that I'm uncertain about how effecient this implementation is relative to where it could be. The promise of JAX is abstraction without the loss of performance, but of course no compiler is perfect. This is something I look forward to exploring further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752844d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import vmap, grad, jit\n",
    "\n",
    "\"\"\"\n",
    "Apply useful transformations to the loss function\n",
    "\"\"\"\n",
    "def make_loss_grad(params):\n",
    "    # transform the loss function into a function that computes its gradient directly\n",
    "    loss_grad = grad(loss_fn)\n",
    "    # \"embed\" the parameters\n",
    "    partial_loss_grad = partial(loss_grad, params)\n",
    "    # parallelize the gradient function\n",
    "    vector_loss_grad = vmap(partial_loss_grad, (0, 0), 0)\n",
    "    # make the function jit-compilation friendly\n",
    "    return jit(vector_loss_grad)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define the gradient-descent process\n",
    "\"\"\"\n",
    "def sgd_vector_minibatch(params, batch_size, learning_rate):\n",
    "    # transform the loss function\n",
    "    loss_grad = make_loss_grad(params)\n",
    "    \n",
    "    # get random subset of observations and responses\n",
    "    batch_x, batch_y = dist_sampler.random_rows(batch_size, Obs, Resp)\n",
    "    \n",
    "    # compute the gradients of loss function relative to the training data\n",
    "    param_gradients = loss_grad(batch_x, batch_y)\n",
    "    \n",
    "    # average the gradients (I suspect including this step in the jit-compiled\n",
    "    # function could yield significant performance gains... tbd)\n",
    "    average_grad_per_param = [\n",
    "        jnp.average(gradients, 0)\n",
    "        for gradients in param_gradients\n",
    "    ]\n",
    "    \n",
    "    # subtract the gradients from the original params and return them\n",
    "    updated_params = [\n",
    "        param - (learning_rate * average_grad)\n",
    "        for param, average_grad in zip(params, average_grad_per_param)\n",
    "    ]\n",
    "    return updated_params\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return the average accuracy of the neural net with the current parameters\n",
    "over a sample of the test data.\n",
    "\"\"\"\n",
    "jit_nn = jit(neural_net)    \n",
    "def avg_prediction_accuracy(params, n=1000):\n",
    "    check_guess = lambda x, y : jnp.argmax(jit_nn(x, params)) == jnp.argmax(y)\n",
    "    vector_nn = vmap(check_guess, (0,0), 0)\n",
    "    xs, ys = dist_sampler.random_rows(n, TestObs, TestResp)\n",
    "    scores = vector_nn(xs, ys)\n",
    "    return jnp.average(scores, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b670d82c",
   "metadata": {},
   "source": [
    "## Training the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy:       8.60%\n",
      "\n",
      "(500) Accuracy:        78.60%\n",
      "\n",
      "(1000) Accuracy:       84.00%\n",
      "\n",
      "(1500) Accuracy:       87.70%\n",
      "\n",
      "(2000) Accuracy:       88.20%\n",
      "\n",
      "(2500) Accuracy:       89.30%\n",
      "\n",
      "(3000) Accuracy:       90.90%\n",
      "\n",
      "(3500) Accuracy:       90.80%\n",
      "\n",
      "(4000) Accuracy:       90.20%\n",
      "\n",
      "(4500) Accuracy:       89.80%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "learning_rate = 0.5\n",
    "iterations = 5000\n",
    "batch_size = 100\n",
    "\n",
    "fmt_acc = lambda acc : f'{(acc * 100):.2f}%\\n'\n",
    "\n",
    "accuracy = avg_prediction_accuracy(neural_net_params)\n",
    "print(f'{\"Initial Accuracy:\":<20}{fmt_acc(accuracy):>10}')\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "for iter_num in range(1, iterations + 1):\n",
    "    # perform gradient descent to get new params\n",
    "    neural_net_params = sgd_vector_minibatch(neural_net_params, batch_size, learning_rate)\n",
    "    \n",
    "    # evaluate accuracy every 500 iterations\n",
    "    if (iter_num) % 500 == 0:\n",
    "        accuracy = avg_prediction_accuracy(neural_net_params)\n",
    "        print(f'{f\"({iter_num}) Accuracy:\":<20}{fmt_acc(accuracy):>10}')\n",
    "        \n",
    "    # decrease the learning rate over time\n",
    "    if (iter_num) % 2000 == 0:\n",
    "        learning_rate *= 0.5\n",
    "        \n",
    "elapsed_seconds = time.time() - starting_time\n",
    "\n",
    "print(f'{elapsed_seconds:.2f} seconds elapsed during training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50297f49",
   "metadata": {},
   "source": [
    "### So there you have it - a simple neural network in JAX!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nns",
   "language": "python",
   "name": "nns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
